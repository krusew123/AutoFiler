AutoFiler â€” Technical Summary (v3.1)

AutoFiler is a Windows-based document processing system that automatically classifies, extracts fields from, and stages incoming documents for downstream processing. The system uses a two-stage architecture that separates fast, generic document intake from type-specific business logic. Stage 1's sole mission is to identify the document type and extract its configured fields. Everything else in the system â€” the review layer, the entity reference, and the Stage 2 corrections feedback loop â€” exists to continuously improve Stage 1's accuracy at that job over time.

Architecture Overview

The system is designed around two processing stages and a learning layer that connects them:

Stage 1 (Core Pipeline) identifies the document type, extracts configured fields, resolves name fields against a unified entity reference, archives the original to a vault, and stages the file with a coded filename and JSON sidecar. The pipeline itself is type-agnostic â€” it doesn't contain branching logic per document type. Instead, each type's configuration drives every step after identification. The goal is maximum throughput bounded only by OCR speed. Stage 1 does not cross-reference fields, generate final filenames, or file to a destination â€” those responsibilities belong to Stage 2.

Stage 2 (Type-Specific Processors) operates downstream. Separate tools consume staged file+sidecar pairs and execute domain logic tailored to each document type â€” cross-referencing, deep fuzzy matching, entity canonicalization, alias management, final naming and filing, and integration with external systems. Each processor runs independently on its own schedule, potentially in parallel per document type, without competing with intake throughput. When Stage 2 detects that Stage 1 extracted a wrong value, it generates a correction that feeds back into Stage 1's configuration through an automated learning loop.

Learning Layer spans both stages and connects them into a continuous improvement cycle. Three mechanisms feed Stage 1's configuration:

1. Review system (human-driven): When Stage 1 can't classify or extract, a two-phase GUI review walks the user through type assignment and field completion. Gap analysis diagnoses why the pipeline missed and suggests classification keywords, content patterns, and extraction patterns. User-approved suggestions are written back to configuration immediately.

2. Stage 2 corrections (automated): When Stage 2 determines a field value was extracted incorrectly, it writes a correction file to a shared Corrections folder. Stage 1's corrections processor picks up the file, locates the correct value in the OCR text, identifies the label context that disambiguates it from other similar values in the document, generates a label-anchored extraction pattern, and persists it to the type's configuration. No human review gate â€” corrections auto-apply with full audit logging.

3. Entity reference growth (organic): Every new entity encountered â€” whether auto-created during pipeline processing, manually entered during review, or corrected by Stage 2 â€” is added to the unified entity reference file. Aliases accumulate, role assignments sharpen, and the dictionary steadily improves name resolution accuracy.

All three mechanisms write to the same JSON config files. Stage 1's code never changes â€” its configuration improves.

Three entry points serve the system. autofiler.py runs a headless watchdog file system observer that triggers process_file() on every new file in the intake folder. autofiler_gui.py provides a tabbed tkinter interface with three tabs â€” Intake (watcher controls), Review (two-phase learning review), and Define (new type creation). review.py launches a legacy CLI review session. Both watcher entry points include deduplication against Windows watchdog duplicate events, path normalization, and file-existence checks before processing. Configuration is entirely JSON-driven with no embedded ML models.

Stage 1 â€” Core Pipeline

The pipeline processes each file in two phases: type identification, then type-driven execution.

Phase 1: Type Identification

Guards reject files before any processing begins â€” zero-byte files, locked files, temp/hidden files, partial downloads, and password-protected PDFs are all caught early and logged without further work.

Classification produces a set of candidate document types by collecting four independent signals. Format detection checks file extensions and MIME types via byte sniffing. Text extraction runs OCR on PDFs, parses .docx files, or OCRs images directly. The extracted text is matched against each type's configured keyword list and regex pattern list. A fourth signal flags types that have both a folder mapping and naming convention configured.

Scoring assigns configurable weights to each signal â€” keyword matches carry the most weight (0.50), followed by pattern matches (0.30), with format and reference matches contributing 0.10 each. The highest-scoring candidate that meets a minimum signal count is selected.

Routing makes a binary decision: if the best score meets or exceeds the confidence threshold (0.75), the file proceeds to Phase 2. Otherwise, it is moved to a review folder for human intervention.

Phase 2: Type-Driven Execution

Once a document type is identified, the pipeline consults that type's configuration and follows its instructions through five steps: extract, resolve, archive, stage, and generate sidecar.

Step 1 â€” Field Extraction. The pipeline extracts all fields defined for the identified type from the OCR text. Each field has a field_type property (defaulting to "text" if absent) that controls extraction behavior. The system supports ten field types: text, date, currency, reference, name, address, phone, email, percentage, and url. For most types, extraction is a single-line regex match â€” the first capturing group is taken as the value. The address type changes extraction behavior: after the initial regex match, the engine scans subsequent lines for address continuations (up to 4 lines), stopping on a blank line or a new label line (text matching the pattern "Word(s): value"). Continuation lines are joined with ", " to produce a single composite value. This handles both same-line addresses ("Bill To: 123 Main St") and label-on-its-own-line addresses where the value starts on the next line. Each type's configuration explicitly declares each field as required or optional:

Required fields must be successfully extracted for the document to proceed. If any required field can't be extracted after resolution (Step 2), the document is rerouted to review. A field may be required because Stage 2 depends on it for cross-referencing, naming, filing, or other processing. The reason is irrelevant to the pipeline â€” the type definition declares it required, and the pipeline enforces it.

Optional fields are extracted on a best-effort basis. Whatever hits is captured; whatever doesn't stays null. No failures, no review triggers. Optional field lists are focused on values that are reliably labeled and extractable via regex â€” not tabular data or layout-dependent content.

Step 2 â€” Field Resolution. After regex extraction, the pipeline runs a field resolver that checks name fields against a unified entity reference file (fieldname_ref.json). Only fields with a reference_lookup configuration are processed â€” currently vendor_name and customer_name. Each reference_lookup declares a role (vendor or customer), and the resolver filters entities by that role before matching. This prevents customer entities from matching vendor fields and vice versa. The resolver handles two scenarios:

When regex extracted a value, the resolver fuzzy-matches it against role-filtered entities in the reference file. If a match is found (threshold 0.80), the extracted value is replaced with the canonical name â€” so "Stericycle Inc" resolves to "Stericycle, Inc." regardless of minor formatting differences. If no match is found, a new entity is auto-created: the raw value becomes the canonical name, a slugified key is generated (e.g., "stericycle_inc"), and the entity is saved with its role and document type code.

When regex missed a required name field, the resolver scans the full OCR text for known entity names, filtered by the field's declared role. This is a two-pass search: first a case-insensitive substring check for exact canonical names and aliases, then a high-threshold (0.95) fuzzy match of each text line against entity names. If an entity is found in the text, the field is filled and removed from the missing list. If not, the field stays missing and will trigger review routing.

The entity reference file is flat â€” no wrapping key. Each entity has a canonical name, an alias list, a roles list (vendor, customer, or both), a doc_types list tracking which type codes have referenced it, and a date_added timestamp. Entities accumulate roles and doc_types across documents: a company first seen as a vendor that later appears as a customer will have both roles.

The reference file starts empty and self-populates as documents are processed. Over time it builds a comprehensive entity dictionary that improves match accuracy â€” new documents benefit from aliases and canonical names established by earlier ones. The resolver produces a resolution_info dict that records the method used for each resolved field (fuzzy_match, text_scan, auto_created, or unresolved), which is included in the sidecar for audit purposes.

Step 3 â€” Vault Archive. The original file is copied to a flat vault directory with a coded prefix. The vault filename format is {3-digit type code}{placeholder}{original filename} â€” for example, an invoice (code 001) named "Stericycle Invoice.pdf" becomes "0010Stericycle Invoice.pdf". The vault preserves the original file exactly as received. Duplicate filenames are handled with timestamp suffixes.

Step 4 â€” Staging. The pipeline generates a coded staging filename from the type's staging_fields mapping. Each type defines which of its extracted fields map to five universal staging slots: vendor, customer, date, reference, and amount. The staging name format is {code}_{vendor}_{customer}_{date}_{reference}_{amount}, where each slot is truncated to a fixed width (vendors and customers left-truncated to 15 characters, references right-truncated to 15, amounts right-truncated to 9, dates converted to YYYYMMDD). Missing slots default to "000". The file is then moved from intake to the flat staging directory with this coded filename.

Step 5 â€” Sidecar Generation. A JSON sidecar is written alongside the staged file in the staging directory, using the same stem as the staged file. The sidecar contains:
- Schema version (1.2), processing timestamp
- Source file path and SHA-256 hash
- Vault file path (archived original location)
- Document type name and 3-digit type code
- Confidence score (null for manually reviewed files)
- All extracted field values, required and optional (post-resolution canonical names)
- Modified (truncated) field values used in the staging filename
- The staging filename itself
- Resolution info for each resolved field (method, raw value, resolved value, entity key, confidence ratio)
- Review info when staged through manual review (review type, original reason, learning applied, manual fields)
- Full OCR-extracted text

The sidecar eliminates redundant OCR on reprocessing and acts as the interface contract between Stage 1 and Stage 2. Both extracted fields (full values) and modified fields (truncated values) are preserved so Stage 2 has complete data to work with. The resolution_info block gives Stage 2 visibility into how each name field was resolved â€” whether it was fuzzy-matched to a known entity, auto-created, recovered via text scan, or left unresolved. The review_info block, when present, documents exactly what human intervention occurred and what configuration learning was applied.

Stage 2 â€” Type-Specific Processors

Downstream processors watch the staging area for new file+sidecar pairs and apply domain-specific logic. Each document type can have its own processor that handles cross-referencing against reference files, deep fuzzy matching, entity resolution, reference canonicalization, final filename generation, filing to destination directories, validation, and integration with external systems. Processors are developed, tested, and evolved independently of the core pipeline and of each other.

Stage 2 manages the reference files that drive entity resolution. This includes matching extracted values against known entities, merging near-duplicate entries, assigning canonical names, building alias lists, and flagging conflicts for human review. Refined entries â€” including new aliases and canonical names â€” feed back into the same reference files, steadily improving matching accuracy on future documents.

A burst of documents hitting intake doesn't stall because one type's processor is doing heavy reference work â€” Stage 2 handles that on its own time.

Stage 2 Corrections Feedback

When a Stage 2 processor determines that Stage 1 extracted a wrong value for a field, it writes a correction file (JSON) to a shared Corrections folder. The correction contains the document type, field name, wrong value, correct value, and a reference to the sidecar (which contains the OCR text). Stage 2 does not modify Stage 1's config files directly â€” it reports what was wrong and what the right answer is.

Stage 1's corrections processor picks up the file and performs label-anchored pattern generation. It loads the OCR text, locates the correct value, examines the surrounding context to identify the label or structural cue that disambiguates it from other similar values in the document, and generates a context-aware extraction regex. For example, if an invoice has four dollar amounts but the correct one follows "Total Invoice Charges", the generated pattern would anchor on that label: Total\s+Invoice\s+Charges\s*\$?([\d,]+\.\d{2}). This pattern is added to the type's extraction field configuration.

New patterns are added alongside existing ones, not replacing them. The label-anchored pattern takes priority when it matches, while generic patterns serve as fallbacks for documents with different layouts. If a generated pattern misfires on a future document, Stage 2 catches the error again and generates another correction â€” the system is self-correcting.

Two categories of corrections flow through this mechanism:

Entity corrections â€” wrong name or value resolution (e.g., "the vendor is Acme Corp, not XYZ Inc"). These update the entity reference file: fixing roles, adding aliases, or creating new entities.

Extraction corrections â€” wrong value captured from text (e.g., "the amount is 172.93, not 02"). These generate label-anchored extraction patterns that teach Stage 1 to find the right value in context.

Both categories auto-apply without a human review gate. The structured log records every correction and every pattern addition, providing a full audit trail. The corrections processor can be triggered by the watcher's polling loop, a GUI button, or a scan on startup.

Review System

Files that don't reach the confidence threshold or are missing required fields after resolution land in a review folder. The review system operates through two phases, each designed to both resolve the immediate file and teach the pipeline to handle similar documents in the future.

Review Queue. A persistent review queue (review_state.json) tracks every file in the review folder with status (pending, in_review, resolved), timestamps, review reason, and current phase. The queue scans the review folder on demand, registering new arrivals as pending and sorting by oldest first. Files that are physically present in the review folder but have stale statuses from previous sessions (resolved or in_review) are automatically reset to pending on scan, ensuring files re-routed to review are always discoverable. Each file's metadata includes the routing reason (no_candidate, score below threshold, or specific missing fields) and tracks progression through review phases.

Phase A â€” Classification Review. When a file's document type couldn't be determined, the user assigns it from a dropdown of existing types or defines a new type via the Define tab. After type assignment, the system runs gap analysis: it compares the file's OCR text against the assigned type's classification configuration to diagnose why the pipeline missed. The gap analyzer identifies which of the type's keywords and patterns matched, which missed, and generates suggestions for new signals.

Suggested keywords are derived by extracting label portions from lines containing colons, identifying capitalized multi-word phrases and ALL-CAPS terms, filtering against stopwords and existing keywords across all types, and ranking by frequency. Suggested patterns are generated by scanning text for structural formats (dates, currency, reference numbers, identifiers) and building generalized regex patterns that aren't already covered by the type's configuration.

The user reviews suggestions through a three-way routing interface. Each suggested keyword can be: skipped (irrelevant), added as a classification keyword (improves future type scoring), or registered as an entity in the entity reference file (with role assignment and optional linking to an existing entity as an alias). Suggested patterns can be individually approved or rejected. Approved changes are written to type_definitions.json and fieldname_ref.json through thread-safe read-modify-write operations.

After classification learning, the system attempts field extraction. If all required fields are extracted, the file proceeds to staging. If not, it transitions to Phase B.

Phase B â€” Extraction Review. When required fields are missing, the system diagnoses extraction gaps. For each missing field, it tests existing extraction patterns against the text, reports which matched and which didn't, and searches for candidate values using field-type-aware heuristics. Date fields get date pattern detection, name fields get proper noun detection, amount fields get currency detection, and so on. Each candidate includes the text snippet, line number, and a suggested capture-group regex.

The user can approve suggested extraction patterns, which are added to the type's field configuration. The system then re-extracts. If fields are still missing after learning, the user enters values manually. Manual name-field values with reference_lookup configuration are automatically added to the entity reference file, building the entity dictionary for future documents.

Every file that enters review exits as a staged document with a vault archive, coded filename, and sidecar â€” identical output format to auto-processed files. The sidecar's review_info block records the review type, original routing reason, all learning applied (keywords added, patterns added, extraction patterns added), and any manually supplied field values.

Define Tab â€” Two-Column Type Creation. The Define tab creates new document types through a two-column layout that separates document analysis (left) from type definition (right). When opened standalone (not from Review), the left column is hidden and only the type definition sections are shown.

When launched from Review during Phase A, the left column receives the document's extracted OCR text and presents two sections:

Extracted Text. A scrollable read-only text widget displays the document's OCR output. The user can select any word or phrase and click a "Population" button to add it to the keyword population below. A search field with up/down arrow buttons (â–²/â–¼) allows navigating through matches in the document text â€” the â–¼ button and Enter key advance to the next match, â–² goes to the previous match, both wrapping around at the ends. All matches are highlighted in yellow simultaneously, with the current match scrolled into view.

Keyword Population. The gap analyzer scans the document text and produces up to 20 ranked keyword candidates (label phrases, capitalized multi-word terms, ALL-CAPS phrases). Each keyword is presented in a row with a trashcan (ðŸ—‘) delete button, a bold keyword label, and three radio buttons under column headers labeled "tags", "extract", and "skip". The radio buttons are unlabeled â€” the column headers above identify each option, with radio buttons centered beneath their corresponding header. The three options are mutually exclusive (single-select): "tags" routes the keyword to classification keywords, "extract" routes it to an extraction field row, and "skip" (the default) marks it for removal. A "Process" button executes the routing: skipped keywords are removed from the population, tags-selected terms are added to classification keywords, and extract-selected terms create extraction field rows. When a keyword is routed to extract, the system auto-detects its field_type from keyword context â€” date-related words map to date, amount/total/balance to currency, number/id/ref to reference, address/remit to/mail to/street/location to address, vendor/supplier to name with vendor role, customer/client/bill to to name with customer role, phone/fax/tel to phone, email to email, percent/rate to percentage, url/website/link to url, and unrecognized keywords default to text. The detected field_type drives pattern generation: date fields get date capture patterns, currency fields get dollar-amount patterns, reference fields get alphanumeric patterns, address fields get a permissive end-of-line pattern (allowing empty captures for label-only lines), phone fields get 10-digit US phone patterns, email fields get standard email patterns, percentage fields get digit-percent patterns, and url fields get http/https URL patterns. The keyword text is prepopulated in the field name entry and displayed as a bold origin label on the field row. Actions are idempotent â€” processing a keyword that was already processed with the same selection does not create duplicates. A write-in entry at the bottom allows adding custom keywords to the population.

The right column contains four sections that are always visible:

doc_type. Required metadata marked with asterisks: Type Name (auto-prepended to the classification keywords on save if not already present), Naming Pattern (default {original_name}_{date}), and Container Formats (comma-separated extensions). Optional fields: Destination Subfolder, Content Patterns (one regex per line), and MIME Types (comma-separated).

tags. A listbox showing classification keywords that drive scoring, with a minimum-hits spinbox (keyword_threshold, default 2), manual add entry, and Remove Selected button. Keywords arrive here from three sources: the keyword population's tags radio selection, manual entry in this section, and the type name auto-prepend at save time.

extract. Dynamic extraction field rows in a grid table with column headers (keyword, field name, field type, patterns, req, opt, name_ref). Each row contains: a keyword origin label in bold blue (shown when the field was routed from the keyword population, empty for manually added fields), a field name entry (prepopulated with the keyword text when routed from population, blank when manually added), a field type dropdown (readonly combobox with ten options: text, date, currency, reference, name, address, phone, email, percentage, url), a patterns entry (semicolon-separated regexes, auto-generated from the field type), req/opt radio buttons (mutually exclusive, req selected by default), a name_ref checkbox, and a trashcan (ðŸ—‘) delete button. Changing the field type dropdown regenerates the pattern automatically using the field name as the label anchor. The field_type is persisted to type_definitions.json on save, stored as a property on each extraction field's configuration alongside patterns and required. The req/opt radio buttons control whether the field is required or optional â€” only one can be selected. The name_ref checkbox operates independently of req/opt and marks a field for entity reference resolution â€” when checked, the field's configuration includes a reference_lookup directive (initially empty), signaling the pipeline's field resolver to process this field against the entity reference. The role can be inferred at processing time from field name context or configured later. The "+ Add Field" button creates a blank row with no keyword label and field type defaulting to text.

Staging Field Mapping. Maps extraction fields to the five universal staging filename slots (vendor, customer, date, reference, amount). Editable comboboxes are populated from both the classification keywords and extraction field names, refreshing automatically when either changes. The user can also type values manually for slots that don't correspond to a named keyword or field.

Validation runs against the full type definition schema before persistence, including field_type validation â€” missing field_type values are accepted (defaulting to text behavior), but invalid values are rejected with an error listing the ten valid types. Save writes to all three config files (type_definitions.json, folder_mappings.json, naming_conventions.json) and reloads the config cache. After save, the Define tab returns to review with the new type pre-selected.

Review Engine. The review workflow is orchestrated by a stateless engine that composes the same lower-level modules used by the automatic pipeline â€” classifier, scorer, content_matcher, field_resolver, gap_analyzer, sidecar, staging_namer, and vault. The engine exposes discrete operations (classify, diagnose classification, attempt extraction, diagnose extraction, stage file) that the GUI calls in sequence. This separation keeps all business logic testable without a GUI and ensures reviewed files go through the same processing path as auto-filed ones.

Configuration

All behavior is driven by JSON config files. settings.json defines paths (intake, staging, vault, review, logs, sidecars), thresholds, and tool locations. type_definitions.json holds the full schema for each document type â€” a unique 3-digit code, formats, keywords, patterns, field definitions with explicit required/optional flags, a field_type property per field (one of ten types: text, date, currency, reference, name, address, phone, email, percentage, url â€” defaults to text if absent), optional reference_lookup directives for name fields (declaring the role to filter against), and a staging_fields mapping that connects universal staging slots to type-specific field names. Reference files under Config/References/ control scoring weights, folder routing, filename patterns, and entity reference data. fieldname_ref.json serves as the unified entity reference shared across all document types and roles. Type-specific reference files are consumed by Stage 2 processors.

The ConfigLoader caches all config in memory and exposes each file as a property. A reload() method clears the cache on every incoming file event so configuration changes â€” including those written by the review system's learning operations or the corrections processor â€” take effect without restarting the service. Config mutations during review and corrections processing use a module-level threading lock to prevent race conditions with the watcher's concurrent config reads.

GUI Application

The desktop application (autofiler_gui.py) provides a tabbed tkinter interface built on ttk.Notebook with three tabs sharing a common context (root window, ConfigLoader, logger).

Intake Tab. The watcher service UI displays a status indicator (green dot when running, gray when stopped), the watched folder path, Start/Stop buttons, and a scrolled activity log. The IntakeHandler uses watchdog's Observer to detect new files, with deduplication (5-second window per normalized file path), a 1-second write-completion delay, and file-existence verification before processing. Activity messages are dispatched to the log widget through thread-safe root.after() callbacks.

Review Tab. The two-phase review interface is driven by a state machine with states: IDLE, CLASSIFYING, PHASE_A, DIAGNOSING_A, LEARNING_A, EXTRACTING, PHASE_B, DIAGNOSING_B, LEARNING_B, RE_EXTRACTING, MANUAL_ENTRY, STAGING, and DONE. The left panel shows the file queue as a treeview with phase indicators. The right panel rebuilds dynamically for each state â€” type assignment dropdowns, gap analysis results with three-way routing controls, extraction diagnostics, manual entry fields, and staging confirmation. Long-running operations (OCR, extraction, gap analysis) run on daemon threads with UI callbacks to prevent freezing. When the user clicks "Define New Type" during Phase A, the document's extracted text is passed through to the Define tab so the new type can be built from the document's actual content.

Define Tab. The type creation form uses a horizontal PanedWindow with two columns. When document context is available (linked from Review), the left column shows two sections: Extracted Text (scrollable read-only preview with a Population routing button, search field, and â–²/â–¼ navigation buttons for cycling through matches) and Keyword Population (up to 20 keywords with three radio buttons per row â€” tags/extract/skip â€” under column headers, and a Process button that removes skipped keywords, routes tags-selected terms to classification, and routes extract-selected terms to field rows with keyword prepopulation and auto-detected field type). The right column contains four sections: doc_type (type name, naming pattern, container formats, and optional metadata), tags (classification keyword listbox with minimum-hits spinbox and manual add/remove), extract (grid-table field rows with column headers for keyword/field name/field type/patterns/req/opt/name_ref, keyword origin labels in bold blue for population-routed fields, field type dropdown that regenerates patterns on change, req/opt radio buttons, independent name_ref checkbox, and trashcan delete buttons), and Staging Field Mapping (editable comboboxes populated from keywords and field names, with manual entry support). All form fields have concise help notes. When opened standalone, the left column is hidden and only the right column sections are shown. Mousewheel scrolling is context-aware, binding to whichever column the cursor is over.

The extracted text preview is read-only but fully selectable, supporting mouse selection, Shift+arrow keyboard selection, Ctrl+A (select all), and Ctrl+C (copy). A single "Population" button lets the user highlight any word or phrase in the document text and add it to the keyword population for triage. A search field with â–² (previous) and â–¼ (next) navigation buttons finds and highlights all matches in the text simultaneously in yellow, scrolling to the current match. Enter in the search field advances to the next match. Both directions wrap around at the document boundaries. From the keyword population, each term is routed through radio buttons â€” to classification keywords (tags), extraction fields (extract), or removal (skip) â€” centralizing all routing decisions in the population rather than through multiple text-selection buttons. This single-path design encourages the user to build up a working set of candidate terms and then triage them systematically rather than routing individually from the text. The radio buttons enforce single-select: each keyword gets exactly one route. The column headers ("tags", "extract", "skip") sit above the radio columns and provide persistent visual context, with radio buttons centered beneath their corresponding header. Skip is the default selection for new keywords.

When a keyword is routed to extract via the Process button, the system auto-detects a field_type from keyword semantics and generates a type-appropriate regex pattern. The resulting field row in the extract section shows a bold blue keyword origin label, a prepopulated field name, the detected field type in a readonly dropdown, and the generated pattern. The user can override any of these â€” changing the field type dropdown regenerates the pattern using the current field name as the label anchor, so the user can experiment with different type interpretations without manually editing regex. Manually added fields (via "+ Add Field") have no origin label, a blank field name, and default to text type. The extract section uses a grid table with column headers (keyword, field name, field type, patterns, req, opt, name_ref) aligned above each column. The req/opt radio buttons are mutually exclusive â€” selecting one deselects the other. The name_ref checkbox operates independently, allowing any combination of required/optional with name_ref on or off. Delete buttons across both sections use a trashcan icon (ðŸ—‘) for consistent visual language.

Logging and Export

Every pipeline action is recorded as a JSON-line entry in a structured log file â€” staging, vault archiving, review routing, field resolution, manual filing, errors, config learning events, review staging, corrections processing, and new type creation. Field resolution events log the method used (fuzzy_match, text_scan, auto_created) along with the raw and resolved values and confidence ratio. Unresolved fields are logged separately with the document type for diagnostic review. Config learning events record which keywords, patterns, and extraction patterns were added during review. Corrections events record the Stage 2 source, the wrong and correct values, and the generated pattern. Review staging events record the review type and any manually supplied fields. Console output mirrors these events through Python's logging module. A CSV export utility can dump reference files to CSV for external use.

Design Philosophy

Stage 1's mission is narrow and clear: identify the document type and extract its configured fields. Everything else in the system exists to make Stage 1 better at that job. The review system teaches Stage 1 when it fails completely. Stage 2 corrections teach Stage 1 when it succeeds partially but extracts a wrong value. Entity reference growth teaches Stage 1 to resolve names it hasn't seen before. All three mechanisms feed the same JSON config files. Stage 1's code never changes â€” its accuracy improves through configuration alone.

AutoFiler prioritizes auditability, throughput, and separation of concerns. Classification is fully deterministic and configurable through JSON â€” no machine learning models. The weighted signal system makes it straightforward to understand why any given file was classified a particular way. The separation between auto-staging and human review, gated by a single confidence threshold, provides a clear safety boundary.

The two-stage architecture keeps the core pipeline fast and simple. Stage 1 follows type-driven configuration rather than containing type-specific code. Its failure modes are straightforward: either OCR worked or it didn't, either the required fields were extracted or resolved or they weren't. There are no partial states where cross-referencing or filing logic left things half-resolved. That complexity belongs in Stage 2 where type-specific processors handle it with full context. Adding new document types means adding configuration and, if needed, a Stage 2 processor â€” not adding branches to the intake pipeline.

The field_type system extends this principle to extraction. Rather than embedding format-specific logic in the pipeline, each field's type is declared in configuration and drives both pattern generation (in the GUI) and extraction behavior (in the engine). The address type is the first field type to alter extraction behavior beyond single-line regex â€” it captures multi-line continuations, recognizing that addresses span multiple lines in most document layouts. The remaining types (date, currency, reference, name, phone, email, percentage, url, text) currently differ only in their auto-generated regex patterns, but the field_type property provides a stable extension point for future type-specific extraction behaviors without modifying existing configuration or code paths. Existing types without field_type set continue to work unchanged â€” they default to text behavior.

The field resolver occupies a deliberate middle ground between Stage 1's regex extraction and Stage 2's deep entity resolution. It performs lightweight canonicalization and recovery â€” enough to prevent unnecessary review routing when a known entity name appears in the document but doesn't match a regex pattern. It does not attempt alias merging, conflict resolution, or cross-document deduplication. Those remain Stage 2 responsibilities. Role filtering ensures that entity matching respects the semantic intent of each field â€” vendor entities match vendor fields, customer entities match customer fields â€” preventing cross-role contamination. The unified entity reference file self-populates through normal document processing, building value over time without manual curation.

The learning flywheel is designed so that every document processed makes the system smarter. Classification reviews add keywords and patterns that improve future scoring. Extraction reviews add capture-group patterns that improve future field extraction. Manual name entries create entity references that improve future name resolution. Stage 2 corrections generate label-anchored patterns that teach Stage 1 to pick the right value when multiple candidates exist in a document. The learning is incremental and auditable â€” each change is a discrete addition to a JSON config file, not a black-box model update. Over time, the pipeline's coverage and precision expand organically as knowledge accumulates from real documents across both stages.

The review system extends this principle to type creation. When a document can't be classified because no matching type exists, the Define tab doesn't start from a blank slate â€” it analyzes the document's actual content and presents what it finds. The keyword population serves as a persistent working area where gap-analysis keywords and user-selected text fragments are triaged through radio buttons. Each keyword gets exactly one routing destination â€” tags (classification), extract (field creation), or skip (removal) â€” enforced by the single-select radio design. When routed to extract, the keyword text carries through to the field row as both an origin label and a prepopulated field name, with field_type auto-detected from keyword semantics and a type-appropriate pattern generated automatically. The user can override the field type via a dropdown, which regenerates the pattern â€” making it easy to experiment with different type interpretations. On the definition side, the type's metadata, classification keywords, extraction fields, and staging mappings are organized in four purpose-specific sections that the population feeds into. The extract section's grid-table layout with column headers, field type dropdown, req/opt radio buttons, and independent name_ref checkbox allow the user to configure field behavior directly during type creation, without requiring a separate configuration step. The user curates what the system found rather than building from scratch, shifting the role from manual transcription to informed curation and making new type creation faster and more accurate since the type definition is grounded in the document's actual content.

The review engine and corrections processor maintain the same processing path as automatic filing. Reviewed files produce identical output artifacts â€” vault archive, coded staging file, and rich sidecar â€” through the same lower-level modules. The sidecar's review_info block provides full provenance for downstream processors to distinguish auto-processed from manually reviewed files and to see exactly what learning was applied.

Stage 1's output â€” a vault archive, a coded staging file, and a rich sidecar â€” gives Stage 2 everything it needs without requiring re-extraction. The coded staging filename provides a consistent, sortable identifier across all document types, while the sidecar preserves both raw and truncated field values alongside resolution provenance. The vault ensures originals are never lost regardless of what downstream processing does.
